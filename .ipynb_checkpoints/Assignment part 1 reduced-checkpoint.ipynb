{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f26cbf3b-affe-4d80-81eb-4f36e0394728",
   "metadata": {},
   "source": [
    "![image.png](mychild.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f446b1-98ff-4411-968b-784d335591a8",
   "metadata": {},
   "source": [
    "## __Assignment number 1__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4385dc-3f98-4390-96db-a9f4037eb694",
   "metadata": {},
   "source": [
    "### __Week 1 challenges__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf469bf-54c3-4867-860d-6791d0dcde47",
   "metadata": {},
   "source": [
    "__Link to ONEDRIVE__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f46f7d0-fa1d-4792-b272-473b6bec67ef",
   "metadata": {},
   "source": [
    "https://universityofstandrews907-my.sharepoint.com/:f:/r/personal/ofh1_st-andrews_ac_uk/Documents/Python%20Assignment%201/Assingment?csf=1&web=1&e=Ceg3tv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b67d222-e27e-417b-8d49-3e295e91958a",
   "metadata": {},
   "source": [
    "__Challenge 1__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861aeb7e-d77f-44c6-8841-2babf2327857",
   "metadata": {},
   "source": [
    "| Road Type | Description | Example |\n",
    "| --- | --- | --- |\n",
    "| Motorway | designed for high-speed vehicular traffic | https://en.wikipedia.org/wiki/M4_motorway |\n",
    "| A-Road | Key-Transit artery | https://en.wikipedia.org/wiki/A9_road_(Scotland) |\n",
    "| B-Road| Minor Road | https://en.wikipedia.org/wiki/B_roads_in_Zone_7_of_the_Great_Britain_numbering_scheme |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4881fb03-4e35-4211-b80a-a1ac458be621",
   "metadata": {},
   "source": [
    "In the challenge above I used the markdown feature as mentioned too construct a table detailing three key types of roadway in the United Kingdom including a brief description and a hyperlink aimed at a relevant Wikipedia article. Proper construction of the table format, led me to a relevant stackoverflow page: https://stackoverflow.com/questions/48655801/tables-in-markdown-in-jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ea69c4-ca99-4933-b0c7-8f7377138dc9",
   "metadata": {},
   "source": [
    "__Challenge 2__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b90f3ce-3b4d-47ae-a316-ab0caf6842ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = float(input(\"number 1: \"))\n",
    "y = float(input(\"number 2: \"))\n",
    "z = float(input(\"number 3: \"))\n",
    "\n",
    "mean = (x + y + z) / 3\n",
    "\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148a26c9-c8a3-4ef7-bb18-a75c5e1c3669",
   "metadata": {},
   "source": [
    "In this above challenge I was tasked with the construction of code which gives the average of three numbers. The participant is prompted to input a set of three numbers of their choice, given in the float format of which allows for decimalisation. I then specificed the function behind the defined term 'mean' which adds all numbers and divides them by 3. The code prints mean once the user has given each input. These techniques were learnt over basic python tutorials - This one for instance https://www.youtube.com/watch?v=kqtD5dpn9C8&ab_channel=ProgrammingwithMosh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501b1948-6abe-432d-b344-28ef4efc24fd",
   "metadata": {},
   "source": [
    "__Challenge 3__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a56960-4916-4af6-bc3d-48e2ec7a9eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = float(input(\"number: \"))\n",
    "\n",
    "if x % 2 == 0:\n",
    "    print(\"The number in question is even\")\n",
    "else:\n",
    "    print(\"The specified number is odd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b19ab0-bb9a-481b-b240-ace7a15dd6ab",
   "metadata": {},
   "source": [
    "Using a similar input prompt as the prior question and a basic if statement, participants are tasked with inputing a float value of their choice. If the input is divisible by two an ouput making note of the numbers even status is recieved. If this ceases to be the output gives a prompt making note of its odd status."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afd8391-60a5-46ef-9900-d04721ef1c80",
   "metadata": {},
   "source": [
    "__Challenge 4__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c8f530-6ccb-4b52-b385-7b8666e8d39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = float(input(\"number: \"))\n",
    "x = int(x)\n",
    "\n",
    "factorial = 1\n",
    "\n",
    "for i in range(1, x+1):\n",
    "    factorial *= i\n",
    "\n",
    "print(f\"The factorial of {x} is: {factorial}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1804c2a2-f561-47f2-b155-bff2edbe3f61",
   "metadata": {},
   "source": [
    "A slightly more complicated loop. The participant is prompted to give a value (X) whereby the code returns the factorial of the inputted value. This is achieved through use pf a for loop which iterates through to a value one higher than the x value giving the amount of times the multiplication should occur. As such the amount of multiplications varies based on the input integer as is the case with a factorial calculation with the loop running indefinately untill the point when it equals x+1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1db64b3-80c3-4ed4-ac19-d583e6ac1848",
   "metadata": {},
   "source": [
    "__Challenge 5__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68bf8cf-ad51-401f-a2d3-f910f201bcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mark = int(input(\"Enter the mark: \"))\n",
    "\n",
    "if mark > 80:\n",
    "    print(\"Grade 20\")\n",
    "elif 60 <= mark < 80:\n",
    "    print(\"Grade 15\")\n",
    "elif 40 <= mark < 60:\n",
    "    print(\"Grade 10\")\n",
    "elif 20 <= mark < 40:\n",
    "    print(\"Grade 5\")\n",
    "else:\n",
    "    print(\"No Grade\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e41f18c-591a-4ab5-b8f7-1ce71b94fa43",
   "metadata": {},
   "source": [
    "This code uses an if, elif, else loop format to sort a given integer into one of five grade catergorys based on the numerical value. The code is based logic that if the given integer is greater on equal to a given numerical boundary and less than the higher bound it will fit within a predefined gradebracket. The correct grade bracket is given thorugh navigating a linearily defined loop structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8bbe9d-5a67-4c5a-9483-5421341c545f",
   "metadata": {},
   "source": [
    "__Challenge 6__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0941ff2-dc4b-41fc-9347-6532a57938d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempf = int(input(\"Enter the temperature in fahrenheit: \"))\n",
    "\n",
    "def conv(tempf):\n",
    "    return (tempf - 32) * 5/9\n",
    "\n",
    "celsius = conv(tempf)\n",
    "print(f\"the temperature in celsisus is: {celsius}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7825685-72f0-4ec9-b877-dd30dc3717f8",
   "metadata": {},
   "source": [
    "This code involves the creation of a function which converts a given fahrenheit value integer and returns a value which has undergone a mathematical equation as specified effectively converting to celsius."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f17e25-0511-48e3-81ba-5ac3a195dfe4",
   "metadata": {},
   "source": [
    "__Challenge 7__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4144120d-8949-4682-998e-522758c582a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "arr = np.random.randint(0,10, size =(1,5))\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fb206d-54c3-4738-84fb-cd1ed2602b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_value = np.mean(arr)\n",
    "mean_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4795b862-354c-419a-bdfd-7d75721b292a",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_deviation = np.std(arr) \n",
    "std_deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001afb4e-f938-48a7-9551-d5bcce2fa839",
   "metadata": {},
   "source": [
    "I have imported the package Numpys for as to create a numpys array, a key feature in data analysis. I have given the array a set of bounds such that it gives an single dimension array consisting of a random set of integers between 0 and 10.\n",
    "\n",
    "The mean and std methods were then used on the array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d2e023-959d-4b70-939b-b433640c840b",
   "metadata": {},
   "source": [
    "__Challenge 8__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bde6ba4-1822-4a13-b99f-062f39010682",
   "metadata": {},
   "source": [
    "the data provided is shown below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d759218f-abe9-497a-8217-69d703436b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sales_data_dict = {\n",
    "    'Date': ['2022-01-01', '2022-01-02', '2022-01-03', '2022-01-04'],\n",
    "    'Region': ['North', 'South', 'East', 'West'],\n",
    "    'Product': ['A', 'B', 'A', 'C'],\n",
    "    'Quantity': [15, 8, 12, 10],\n",
    "    'Revenue': [1500, 1200, 1800, 900]\n",
    "}\n",
    "\n",
    "sales_data = pd.DataFrame(sales_data_dict)\n",
    "\n",
    "\n",
    "movie_ratings_dict = {\n",
    "    'Title': ['Movie1', 'Movie2', 'Movie3', 'Movie4'],\n",
    "    'Genre': ['Action', 'Drama', 'Comedy', 'Thriller'],\n",
    "    'Year': [2015, 2010, 2018, 2011],\n",
    "    'Rating': [8.5, 7.8, 9.2, 8.0]\n",
    "}\n",
    "\n",
    "movie_ratings = pd.DataFrame(movie_ratings_dict)\n",
    "\n",
    "print(sales_data)\n",
    "print(movie_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568b1eab-19a2-4f5b-be22-6563a4940f66",
   "metadata": {},
   "source": [
    "showing the first five rows of sales data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b577402-76dd-4b4a-9356-4078f450a691",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.head(5) #there seems to only be 4 rows - unsure what is being asked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34aa926-ea18-4f7f-8d6f-489c25647c71",
   "metadata": {},
   "source": [
    "Using the head function I returned the first 5 rows, being the headers and four succesive rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4965256-a3c9-4d96-bfa7-83a31e085a27",
   "metadata": {},
   "source": [
    "show only rows where quantity exceeds 10 and the region is North"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86940a3a-23de-420b-9518-d4c35f9291ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.loc[(sales_data['Quantity'] > 10) & (sales_data['Region'] == 'North')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d18598c-5161-4be9-b2ba-248bca623981",
   "metadata": {},
   "source": [
    "Used loc method to select row where quantity exceeded 10 and sales region is North"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5828ea12-6855-4f49-9f6d-7d0f5ede49c2",
   "metadata": {},
   "source": [
    "show only first row and first three columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593031ae-803a-4d46-ba65-cd92acadaaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.iloc[0, 0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cbb35e-8042-4ffe-8b45-b2f086cba7a1",
   "metadata": {},
   "source": [
    "Use of the iloc function to extract the specific value of the first 3 attributes in the 1st row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48845576-1322-4a08-94fe-5d91a8394ce5",
   "metadata": {},
   "source": [
    "total sales for each region and display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8e0def-c8bd-4953-ac72-c479189a25ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sales = sales_data.Revenue / sales_data.Quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb51610-81aa-41e9-9686-c4c24f063e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcab9eb5-f63c-4d4d-8328-365d625708f1",
   "metadata": {},
   "source": [
    "i've taken the total sales to be equivalent to the revenue over the quantity giving the amount of product sold. They're listed as such above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e426fe9-eead-484b-bcad-065cf784053f",
   "metadata": {},
   "source": [
    "last three rows of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23ff0fe-0abd-4a0b-9ec9-ab017571fa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_ratings.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295f17e9-6fc5-4a92-bacc-4433155dda82",
   "metadata": {},
   "source": [
    "The tail function was used to recive only the last three entrys, Additionally I've also calculated the mean rating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb334159-0775-48d1-a2f8-471a7a4c01bd",
   "metadata": {},
   "source": [
    "Calculating and displaying the average rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0466dea9-7ffb-427f-851d-730c96b51813",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_ratings['Rating'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfb9b3e-0661-49c5-aa5a-e8e127365ea1",
   "metadata": {},
   "source": [
    "__Challenge 9__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8182c6d7-d622-4f32-975e-a8de255289c4",
   "metadata": {},
   "source": [
    "This is the intial instance upon which we have loaded up the ol Geopandas package, a critical element of the GiS world. Using the described shapefile_path I have loaded in data showing the location of road traffic accidents. I have read the file as a Geopandas Geodata frame such that it has a geometry column, set to the typical CRS of EPSG:4326 while using the \"NUMBER_OF_\" column to visually demonstrate in the typical chloropleth fashion areas where mroe cars were involved in accidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5babe4d9-cd3e-4df3-bfed-26e6b0cab343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "\n",
    "shapefile_path = 'Week 1 Challenge data/Road_Safety_Accidents.shp'\n",
    "gdf = gpd.read_file('Week 1 Challenge data/Road_Safety_Accidents.shp')\n",
    "gdf.explore(column='NUMBER_OF_', cmap='Reds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de98331-4677-4111-83db-a44cb5d50e14",
   "metadata": {},
   "source": [
    "#Challenge 10 ommitted on account of it being associated with push/pulling GitHub in the original thingy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d8cbbe-9e68-42e0-989a-21cac24476c9",
   "metadata": {},
   "source": [
    "## __Week 2 Challenges__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cda7e4-b7db-4a47-8fff-e7297e238ea9",
   "metadata": {},
   "source": [
    "__Challenge 1__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e69a8f-b45d-45ea-b90f-2423cb90cc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#1\n",
    "\n",
    "a = np.random.randint(0, 40, 100)\n",
    "b = np.random.randint(0, 40, 100)\n",
    "c = np.random.randint(0, 40, 100)\n",
    "d = np.random.randint(0, 40, 100)\n",
    "\n",
    "e = pd.DataFrame({'a':a,'b':b,'c':c,'d':d})\n",
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af3981b-6465-4b6f-8327-0fffa94bd250",
   "metadata": {},
   "source": [
    "I have created a pandas dataframe with a set of four columns, labelled a,b,c and d respectively. Each column is 100 rows in length, including the title with each value being a random integer between 0 and 40. This dataframe is labelled e and is as above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8659bcd-b317-445e-a98f-c84eabf9835f",
   "metadata": {},
   "source": [
    "Use of the iloc feature allows for the identification of specific values within a pandas dataframe. In this instance f is an array constituting the first 30 rows and the first 3 columns. This is subsequently converted into a dataframe giving us that nice clean look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec95185-6745-4ee1-bd3f-61d473d71672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#2\n",
    "\n",
    "f = e.iloc[0:30,0:3]\n",
    "subset_df = pd.DataFrame(f)\n",
    "subset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8742306-3e39-4ecb-bb2f-0ef6e24276a2",
   "metadata": {},
   "source": [
    "The code here demonstrates the creation of a new geopandas dataframe in which each of the four columns a,b,c and d are filtered such that the values have to exceed 20. These values are lifted from the previous dataframe 'e'. The bracket/square bracket component of the code lifts each of the four seperate columns individually and filters them. Following this the column are renamed to a new value and the final dataframe filters specifically column 1 from the subsidarary product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14179570-3c51-415a-a28e-464927afa796",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "\n",
    "g = e[(e['a'] > 20) & (e['b'] > 20) & (e['c'] > 20) & (e['d'] > 20)]\n",
    "g\n",
    "\n",
    "g = g.rename(columns={'a': 'column 1', 'b': 'column 2', 'c': 'column 3', 'd': 'column 4'})\n",
    "\n",
    "filtered_df = g[['column 1']]\n",
    "\n",
    "filtered_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1e4d52-455c-43cf-a249-3ada6ea55529",
   "metadata": {},
   "source": [
    "Below are examples of basic statistical method use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a86d51-ec56-4add-adf0-726b9527e27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31739028-f1cd-400b-92ab-50284827db97",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.std()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8219bd1-c046-48eb-a742-ddbe6334f6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.groupby(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfa4ae9-49f6-4c68-aa52-c2f08192002c",
   "metadata": {},
   "source": [
    "__Challenge 2__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc1063f-8009-4bfe-b466-30da0292f0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# Let's describe the url, it is usually easier to do it like this, so in the future, you can easily update the URL\n",
    "url_bikes = \"https://api.glasgow.gov.uk/mobility/v1/get_rentals?startDate=2022-05-01&endDate=2023-05-01\"\n",
    "# Making the query to the web server, using the Get method from the requests library \n",
    "response = requests.get(url_bikes)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82e3fad-c31f-445b-8cc3-129f4643a965",
   "metadata": {},
   "source": [
    "I am requesting data located in the cloud via an API requested. Specifically this data is derived from the Glasgow Open Data store. The Response [200] means everything worked properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73206158-1ca8-49af-aa8c-8892d4a118ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a1fa6b-31f8-4352-a177-73ca57cd2f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "rental_data = data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc4488c-7cde-4845-9d68-a830f35231f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rental_pd = pd.DataFrame(rental_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cce6ad3-f120-4d21-b227-b9f44abe9a17",
   "metadata": {},
   "source": [
    "Here we are filtering out the API formated data to extract only the portions needed for the analysis while creating a new dataframe under the heading rental_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f3572e-208b-4992-ac05-5cefff87168b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_in_column_Lat = rental_pd['endPlaceLat'].isna().any()\n",
    "nan_in_column_Long = rental_pd['endPlaceLong'].isna().any()\n",
    "\n",
    "print(nan_in_column_Lat,nan_in_column_Lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e02a3f-ed9f-4682-a0eb-5b40fd31c443",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_rental_pd = rental_pd.dropna(subset=['startPlaceLat', 'startPlaceLong', 'endPlaceLat','endPlaceLong'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7306da3-b293-4644-ad82-2c1a5b81e93a",
   "metadata": {},
   "source": [
    "Upon finding out that there exist several Na values hidden among the dataframe we go about cleaning the dataset by removing any NA outputs, dropping NAs from the geometry columns associated with the Bike rental location points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0f3cbc-bc14-401c-b956-bbc9fe2a4162",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_bikes_start = gpd.GeoDataFrame(clean_rental_pd, geometry=gpd.points_from_xy(clean_rental_pd['startPlaceLong'], clean_rental_pd['startPlaceLat']))\n",
    "gdf_bikes_end = gpd.GeoDataFrame(clean_rental_pd, geometry=gpd.points_from_xy(clean_rental_pd['endPlaceLong'], clean_rental_pd['endPlaceLat']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e414e65f-8521-4c8e-a997-30e2077d1251",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_bikes_end = gdf_bikes_end.set_crs(\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf37c83-ec4e-4a2b-8c99-cd34b1935f33",
   "metadata": {},
   "source": [
    "I make sure to set the CRS tp EPSG:4326 to ensure my outcome appears correctly on the final map outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f71aa4c-6591-41a9-b590-f01ac436792d",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_cols = [\n",
    "    \"endDate\",\n",
    "    \"endPlaceId\",\n",
    "    \"endPlaceName\",\n",
    "    \"durationSeconds\",\n",
    "    \"isInvalid\",\n",
    "    \"price\",\n",
    "    \"isEbike\",\n",
    "    \"endPlaceLat\",\n",
    "    \"endPlaceLong\",\n",
    "    \"geometry\",\n",
    "]\n",
    "gdf_bikes_end = gdf_bikes_end[keep_cols]\n",
    "gdf_bikes_end.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99064a5e-1da5-4dcf-be4c-aac517f768f5",
   "metadata": {},
   "source": [
    "I am cleaning the data set to ensure only the relevant data is kept in my dataframe - avoid confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df00ff57-9e5b-4d1f-bef8-b963bf620de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_bikes_end.endPlaceId = gdf_bikes_end.endPlaceId.astype(int)\n",
    "gdf_bikes_end.endPlaceName = gdf_bikes_end.endPlaceName.astype(str)\n",
    "gdf_bikes_end['endDate'] = pd.to_datetime(gdf_bikes_end['endDate'], format='%Y-%m-%dT%H:%M:%SZ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8cdc72-a6ef-4205-a046-e30edb1de769",
   "metadata": {},
   "source": [
    "Upon checking the data types of several the data catergories some flag up as objects which aren't usable for further analysis. As such I turn the numerical points into intergers and word ourcomes into strings. Additionally a custom format is used to display time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b427ea-67be-4cee-bdf4-dcb777549b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "num_clusters = 4\n",
    "\n",
    "kmeans_collection = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "gdf_bikes_end['kmeans_cluster'] = kmeans_collection.fit_predict(gdf_bikes_end[['endPlaceLong', 'endPlaceLat']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53125870-3352-497c-ba38-0ec87d8a6193",
   "metadata": {},
   "source": [
    "The KMeans method is imported, as to allow for K-means clustering analysis. This will allow use to visualise the fashion in which cycle hire drop of points are distributed. The number of clusters beinf specified as 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f42410-1201-4002-9215-730ee8c655c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import leafmap\n",
    "\n",
    "m = leafmap.Map(center=(55.860166, -4.257505),\n",
    "                zoom=12,\n",
    "                draw_control=False,\n",
    "                measure_control=False,\n",
    "                fullscreen_control=False,\n",
    "                attribution_control=True,\n",
    "                   \n",
    "               )\n",
    "\n",
    "m.add_basemap(\"CartoDB.Positron\")\n",
    "m.add_data(\n",
    "    gdf_bikes_end,\n",
    "    column='kmeans_cluster',\n",
    "    legend_title='Clusters',\n",
    "    cmap='Set1',\n",
    "    k=4,\n",
    ")\n",
    "\n",
    "#Ploting the map\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a8e9ae-181c-4fba-9e3a-08866fafa13c",
   "metadata": {},
   "source": [
    "Leafmap is a cartographic visualisation tool which is handy for data analysis. The above code creates a new map leafmap plot titled m which is derived from a set of manually set parameters. These include the coordinates and zoom attributes in addition to factors which shape the map visualisation itself including basemap themes and data types. Kmeans are used to effectively demonstrate the location of distinct groupings with the effect of creating a map with colours being derivative of proximity to the nearest of four defined mean central values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e57e5a7-b62c-40db-b418-d5a229745494",
   "metadata": {},
   "source": [
    "__Part2__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f7ac9f-a37e-49e2-83d6-bbdaf00a2d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from geopandas.tools import sjoin\n",
    "\n",
    "# Define the URL for the sensors data\n",
    "url_sensors = \"https://api.glasgow.gov.uk/traffic/v1/movement/sites?null=3_weeks_ago\"\n",
    "\n",
    "# Making the query to the web server using the Get method from the requests library\n",
    "response = requests.get(url_sensors)\n",
    "sensor_data = response.json()\n",
    "# Print the response to check the status\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f597bf-e4cc-4dc4-8b17-5666531a46ed",
   "metadata": {},
   "source": [
    "Above is another API request for the location traffic sensor locations with the data being sourced from 3 weeks prior to the request on the Glasgow Open Data website The data is then filtered as to obtain just the neccesary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94261537-07e8-43ba-ba3a-fe773d6b41cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b889804c-b52c-4132-a762-03e2bdea694f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_df = gpd.GeoDataFrame(columns=['siteId', 'latitude', 'longitude'])\n",
    "\n",
    "for sensor in sensor_data:\n",
    "    site_id = sensor['siteId']\n",
    "    lat = float(sensor['from']['lat'])\n",
    "    lon = float(sensor['from']['long'])\n",
    "    sensor_df = pd.concat([sensor_df, pd.DataFrame({'siteId': [site_id], 'latitude': [lat], 'longitude': [lon]})])\n",
    "\n",
    "sensor_df['geometry'] = gpd.points_from_xy(sensor_df['longitude'], sensor_df['latitude'])\n",
    "\n",
    "sensor_gdf = gpd.GeoDataFrame(sensor_df, geometry='geometry')\n",
    "\n",
    "sensor_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d603011-e635-4ae6-b91e-64cef1637a43",
   "metadata": {},
   "source": [
    "This specific section was quite the headache as the JSON format was provided in a format which requires a for loop to extract the specified portions of data. I have given the response the name sensor_data, however the data output is messy and will have to be cleaned to give only the attributes I shall need for analysis. Thus I define a new pandas dataframe which includes columns only for the geometry and siteId. I then initiate a loop taking the values from each subsequent datapoint in  the JSON response, as specified by the parameters. Within the data two sets of 'lat' and 'long' are provided 'from' and 'to', with 'from' being taken. Sensor_df was then created throughthe concat function which combines the given variables into a datframe output in accordance with the specified column titles. I then created a distinct geometry column which uses the points_from_xy function https://geopandas.org/en/stable/docs/reference/api/geopandas.points_from_xy.html to combine the seperate latitude and longitude columns toward a single column which is used upon creation of a seperate geodataframe, as the geometry key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f54b18-35a7-4e0e-8bae-22bd61d2fe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_gdf.crs = 'EPSG:4326'\n",
    "keep_cols = [\"siteId\", \"geometry\"]\n",
    "sensor_gdf = sensor_gdf[keep_cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97905ae4-fe54-4ad5-b186-526fbec6218c",
   "metadata": {},
   "source": [
    "Set the crs as EPSG:4326 to allow for projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf49380-1d60-408e-9e4a-7b653c90d362",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapefile_path = \"Week2/workplacezones2011scotland/WorkplaceZones2011Scotland/WorkplaceZones2011Scotland.shp\"\n",
    "gdf_shapefile = gpd.read_file(shapefile_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46990846-33a4-4b2b-9eff-089a3ba96d2f",
   "metadata": {},
   "source": [
    "I then imported the given Workplacezones shp file, provided to us from class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45afcefc-9618-49f6-9fb0-9a2d5798d3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_shapefile.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d8159f-55a1-453d-aa12-5652e0523ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_shapefile.to_crs(\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c198ee57-12b8-45e1-b8f4-e45e60c6d2fe",
   "metadata": {},
   "source": [
    "Set the Workplaces shape file to the same crs as our sensor data to allow for combined analysis on the same map plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b651b0a-125d-479c-a802-bd3d8cedc06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LADCD_value = 'S12000046'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84bb7ca-d9fe-4d86-9945-0db9d36249f2",
   "metadata": {},
   "source": [
    "Admittedly I opened up the data on a seperate QGIS tab to gain insight into the relevant LADCD_value being 'S12000046'. This was done by opening the shapefile, projecting it and using the select by cursor feature. I then simply dragged my cursor over an area in central Glasgow. I'm sure theres a more effecient way you could do this in Python..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8816b5-b332-4a86-a197-dd6cff85754a",
   "metadata": {},
   "outputs": [],
   "source": [
    "GlasgowArea = gdf_shapefile[gdf_shapefile['LADCD'] == LADCD_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b97e68-023a-4120-960f-bb1324a09cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "GlasgowArea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d594fab-dcb6-406a-b31c-0e882d753776",
   "metadata": {},
   "source": [
    "I create a seperate shapefile which only shows workingzones for Glasgow itself using the previously defined filer 'LADCD_value' as to apply a filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a735e79b-5bee-499a-8199-00512cf4f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "GlasgowArea = GlasgowArea.to_crs(\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f768f8b-366d-4288-89d7-230c622e5a54",
   "metadata": {},
   "source": [
    "The 'to_crs' method is applied to 100% ensure that the data will be okay to view with the explore method. I've a habit of perhaps doing this too many times when I'm not so sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c43838e-ca28-4ea6-8ddf-a860f69ff3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "GlasgowArea.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471c6aca-cf2b-4d98-947d-0433ec394b53",
   "metadata": {},
   "source": [
    "I had some issues when it came to preserving the entire Glashgow workingarea shapefile. I intially used the spatial joins 'inner' tecnique, however this seemed to crop out the majority of the shapefile WZCD polygons as the traffic sensors were located in rather specific areas in the city centre, or along key trasit routes. As such upon researching the way spatial joins work it was deduced that a left side spatial join could be used to preserve the entire shape file, as oppsoed to cropping unsued areas. The effect was such that an abundance of NaN values are given which were converted to float 0 values in both the index_right and siteId columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e571ae-6f36-41c3-8233-e7f9eecb436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "areawithsensor = gpd.sjoin(GlasgowArea, sensor_gdf, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9e1e04-bf5f-474e-92bc-072cbb5c5e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "areawithsensor.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbdfc3d-e149-4964-bef5-d35290118240",
   "metadata": {},
   "outputs": [],
   "source": [
    "areawithsensor.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aefa70-3348-4843-942e-bb6a33f8c27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "areawithsensor.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c44560-f8e0-456d-9113-df6a2021c51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "areawithsensor.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b870f8f6-8cec-4818-91e7-181d3411edaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_per_WZCD_siteId = areawithsensor.groupby('WZCD')['siteId'].nunique().reset_index(name='unique_siteId_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1a9c9d-3bd9-4b7d-8f68-8a313515e03b",
   "metadata": {},
   "source": [
    "I then define a new GeoSeries which counts the number of unique traffic sensors with the same WZCD and groups them together forming a new column, 'count_per_WZCD_siteId' which is grouped by the WZCD and site Id values as to ensure the correct columns host the appropriate outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6688752e-3ff7-4238-bcbe-88fa13057a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "geom = areawithsensor \n",
    "sensorcount = count_per_WZCD_siteId \n",
    "\n",
    "geomap = pd.merge(geom, sensorcount, left_on=\"WZCD\", right_on=\"WZCD\")\n",
    "\n",
    "geomap.head()\n",
    "\n",
    "keep_cols = [\n",
    "    \"WZCD\",\n",
    "    \"geometry\",\n",
    "    \"unique_siteId_count\",\n",
    "]\n",
    "geomap = geomap[keep_cols]\n",
    "geomap.head()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "geomap.explore(column='unique_siteId_count', cmap='Blues')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f60b30-53f0-42ad-9d0e-5331433a041b",
   "metadata": {},
   "source": [
    "I then define the paramters through which a chloropleth map demonstrating the distribution of traffic sensors in Glasgow is to be created with the output highlighting areas where sensors are found, as well as those where they are not, a process which took me a bit of research to figure out. The Blues 'cmap' is favoured as a feel it provides a nice distinction to those areas with particularly high siteID counts even when zoomed far from the layers.\n",
    "The code highlights the two values of interest being the geometry column and the measured variable, in this case the number of traffic sensors and incoporates them into a map using the geomap function. The format itself was provided in class, however I can't seem to figure out why the areas purportedly having no traffic sensors showing a compltely transparent chloropleth outcome are labelled as having a siteId count of 1. This is rather strange."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5db7ea-4ef5-40bb-8640-966a9fb7b7e6",
   "metadata": {},
   "source": [
    "## __Week 3 challenges__ ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bab44ca-114e-48cc-83bc-8769aaeab091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "\n",
    "listings =pd.read_csv(\"Week3/data/listings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49013a3b-a8bb-4b11-ad49-c0d0db92d159",
   "metadata": {},
   "source": [
    "Here I am loading in some data data provided in class as to be able to run the first of the Week 3 Challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b246990-ba63-46d1-8485-9e23b2ec0f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_listings = listings[['id', \n",
    "                            'neighbourhood_cleansed',\n",
    "                            'latitude',\n",
    "                            'longitude',\n",
    "                            'property_type',\n",
    "                            'room_type',\n",
    "                            'bedrooms',\n",
    "                            'price',\n",
    "                            'number_of_reviews']]\n",
    "\n",
    "subset_listings.loc[subset_listings.index, 'price'] = subset_listings['price'].replace('[\\$,]', '', regex=True).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa380705-94d3-4681-b6b5-aa378cab1223",
   "metadata": {},
   "source": [
    "Cleaning the data as provided in the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170fb696-54ac-4182-a0b1-c4aebf40a0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_listings = subset_listings.dropna()\n",
    "\n",
    "# Let's see the results.\n",
    "subset_listings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4788613-4209-4c1e-9707-611bc7ceddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_description(x):\n",
    "    numerical = x.select_dtypes(include=['int64', 'float64'])\n",
    "    stats = {'Mean': numerical.mean(), 'Count': len(x)}\n",
    "    return pd.DataFrame(stats)\n",
    "\n",
    "data_description(subset_listings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75025723-f9ec-4cb2-99a6-cf82ab7c9338",
   "metadata": {},
   "source": [
    "Here I was tasked with creating a new function using the format provided in class, however specified to include only numerical columns. Using the format provided in the third lab however with a minor alteration, I define numerical as a function which selects only integer and float values while stats creates columns Mean and Count which take the mean and length of each attribute of x. Returned is a pd.Dataframe of stats from x, which is in this instance data_description."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eed46eb-4683-479b-8949-9456a5e4de78",
   "metadata": {},
   "source": [
    "__Challenge 2__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7444fb-cb49-41f5-9cc1-25e0ef928792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "sqorl =pd.read_csv(\"Week3/Sqorl.csv\")\n",
    "\n",
    "sqorl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4df86d8-7212-4ca5-9bd2-978793bdc2f4",
   "metadata": {},
   "source": [
    "Here I am importing a set of data from the NYC open data hub using the traditional csv method as it proved rather difficult and time consuming to find appropriate API datasets with longitude/latitude data. I immediately subset only the geometry and one variable. 'Hectar Squirrel Number' indicates the order in which squirrel was found by the researcher with higher total values being indicative broadly to a higher squirrel population in the sample site as opposed to having much bearing over the specific squirrel indvidual. I like this dataset as it provides an analysis of a contested public good which various actors have claims. An Analysis of squirrel numbers and distribution should dictate measures by conservationalists and local government toward how the park layout and access should be sculpted with urban parks having a role to play as both a recreational space and as a rare haven for wildlife in densely populated urban spaces. While realistically not all urban green space should prioritise wildlife given the range of alternative uses, - playgrounds, fitness etc. it helps to know where species cluster and if there are any potentially removable barriers as to manage habitat and population numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b533278-67dc-4781-8159-2d72d4acb533",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqorl_s = sqorl[['X', \n",
    "                            'Y',\n",
    "                            'Hectare Squirrel Number',\n",
    "                            'Lat/Long']]\n",
    "\n",
    "sqorl_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee7bde2-7430-4601-9620-4f27b56e336f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqorl = gpd.points_from_xy(sqorl_s['X'], sqorl_s['Y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa853bc-8022-4f0f-8367-19f5048eb131",
   "metadata": {},
   "source": [
    "Again using the points from XY method to obtain a single combined geometry column from a pair of seperate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318e9a95-344e-481e-94a9-1bea96e07847",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqorl = gpd.GeoDataFrame(\n",
    "    sqorl_s, geometry=gpd.points_from_xy(sqorl_s.X, sqorl_s.Y), crs=\"EPSG:4326\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c05217-915b-4ded-b1a9-1e33e667cb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqorl = sqorl[['Hectare Squirrel Number',\n",
    "                            'geometry']]\n",
    "\n",
    "sqorl.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d814addb-d8a3-492d-8df2-3839657c3c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqorl.explore(\"Hectare Squirrel Number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a889537d-a28d-4f28-8cb7-62436eb1e9bc",
   "metadata": {},
   "source": [
    "Using the explore function ive created "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5be87e-28e5-4064-af47-8d556e896e00",
   "metadata": {},
   "source": [
    "Upon cleaning the data I am left only with the key variable and the associated geometry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb0520e-2514-4471-959b-26233a7f6ea5",
   "metadata": {},
   "source": [
    "Next I project the data to EPSG32118 which is a suitable projected crs for New York State and surrounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421d17c7-6279-4d41-afed-3bc9df6b1257",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqirl = sqorl.to_crs(\"EPSG:32118\")\n",
    "sqirl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f837cab-3ed4-417e-92e3-f3118285eb31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15689d1-928b-48cc-be27-c71cd4735911",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqirl_b = sqirl.buffer(200)\n",
    "sqirl_b\n",
    "sqirl['geometry'] = sqirl.buffer(200)\n",
    "sqirl\n",
    "sqirl_2 = sqirl\n",
    "sqirl_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1d4093-c22e-40da-8f7d-bbb069913483",
   "metadata": {},
   "source": [
    "Next I create a set of buffers to allow for some funky analysis, allowing for insight into where the squirrels might dwell and in what capacity they tend to prefer specific areas over a more disperate spread. 200 metres seems appropriate given the sheer scale of central park. Was I investigating a slightly less huge urban green space it may have made sense to reduce the buffer size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c049f3ad-e865-4d6b-b355-a2b1cb0b795b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqirl_b.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71be8368-60b4-4ebf-a875-7764886f74e2",
   "metadata": {},
   "source": [
    "Here I am assigning a specific buffer ID to each of the points to allow for some analysis later down the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7e6dfd-63c4-4130-aaaf-78a8d70df222",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqirl_b = gpd.GeoDataFrame(geometry=sqirl_b)\n",
    "sqirl_b['buffer_id'] = range(len(sqirl_b))\n",
    "sqirl_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bee9c64-8603-478e-95b5-99b44a4e8332",
   "metadata": {},
   "source": [
    "The next step is to join the squirrel points to the buffers they lay in so I can give a numerical output delimiting the amount of squirrels per buffer. Naturally as each buffer contains many adjacent squirrel location points this new dataframe returns an absolutely insane amount of row outputs which will not be included in the final dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb3fea2-2e0b-4e32-9b60-8b7bc1b59dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_result =  sqirl.sjoin(sqirl_b, how=\"inner\", predicate='intersects')\n",
    "spatial_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c03d2c-7187-419c-a5f6-66c80fdb0301",
   "metadata": {},
   "source": [
    "Upon grouping the buffer id and the point geometries I produce a new dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab480b47-49d6-4111-af32-a925b950f847",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqirl_per_buffer = spatial_result.groupby('buffer_id').size().reset_index(name='sqirl_per_buffer')\n",
    "sqirl_per_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f0ebc9-5414-4f80-babb-e329f0ced08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(sqirl_per_buffer.sqirl_per_buffer, bins=30, edgecolor='black')\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of sqirl_per_buffer')\n",
    "plt.show\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f4ed77-7416-41ab-9be2-0069f64c373b",
   "metadata": {},
   "source": [
    "This code produces a histogram plot detailing the frequency of 'squirrels per buffer' with the chart pointing to the fact that most buffers see heavy overlaps with others, peaking near 400 mark. Thus we can denote a circumstance in which principally the squirrel population of Central Park resides, concentrated in specific locations among others of the species.\n",
    "Upon including a histogram plot using matplotlib clearly noted is a trend toward squirrels agglomerating among their own kind evidencing a tendency toward concentration in speficic habitat spots. This holds ramifactions for park development and policy in that such areas ought to be safegaurded. Additionally one can note an obscure uptick in values for the first bin such that there may be a few instances of solitary indviduals of whom reside in pottentially peripheral habitat spots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91046b4-1fc4-40d1-9686-2c77c5d9d781",
   "metadata": {},
   "source": [
    "In order to complete some multivariate analysis of the squirrel population I've imported a shape file demonstrating the distribution of drinking fountains in central park. I was wondering whether there was a relationship between the animal population and drinking fountain locations. With assumption that drinking fountains draw human footfall it would be intersting to see whether Squirrels have a preference toward human concentrations or tend to avoid people. Such an understanding would have ramifactions for the way in which rangers and policymakers sculpt the park layout toward managing the squirrel numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d499f8-2b08-4281-a2f7-a1f5064bbaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "shapefile_path = \"Font/Fount.shp\"\n",
    "Fount = gpd.read_file(shapefile_path)\n",
    "\n",
    "Fount = Fount[['PropName','geometry']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a128078-59af-4829-a6f3-6bddcc50a21e",
   "metadata": {},
   "source": [
    "Here we import the shapefile containing the fountains. NB I've fiddled around with this on QGIS to remove some attributes etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436efe17-1a5f-4b0c-9f5e-7f7285c7b008",
   "metadata": {},
   "outputs": [],
   "source": [
    "CPF = Fount[Fount['PropName'] == \"Central Park\"]\n",
    "CPF = CPF.to_crs(epsg=32118)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99812101-45a2-43f0-be89-824b8e9c0612",
   "metadata": {},
   "source": [
    "The CRS is set to a US Based projection and all those fountains outside Central Park are filtered out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ebc1eb-fdcb-44a7-8239-2c40fc15a363",
   "metadata": {},
   "source": [
    "I then complete a join with the squirrel buffers to obtain the number of fountains in each buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9a3594-a7e4-4243-8583-a92683f40662",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_result2 =  CPF.sjoin(sqirl_b, how=\"right\", predicate='intersects')\n",
    "spatial_result2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62064110-b55f-45d0-97ab-177a82871e8d",
   "metadata": {},
   "source": [
    "Upon grouping by buffer id, we can see that an output is given associated with the number of fountains per buffer. Additionally I gave buffer areas with no fountains of which had the value NaN the value of 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2aac91-5b59-4185-beed-e4da850203c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fount_per_buffer = spatial_result2.groupby('buffer_id').size().reset_index(name='fount_per_buffer')\n",
    "fount_per_buffer\n",
    "\n",
    "fount_per_buffer.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22118d8-27ef-46f2-8946-11203da7620b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.merge(fount_per_buffer, sqirl_per_buffer, on=\"buffer_id\")\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eaebf2-6b43-4904-88ca-b760dbd0cffa",
   "metadata": {},
   "source": [
    "Merging the two dataframes on the value buffer id we can perfom multivariate analysis by creating a scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e16cc2a-4d3f-48dc-b762-a6e272134ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.plot.scatter(\"sqirl_per_buffer\", \"fount_per_buffer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bd5c70-30e5-448a-92f4-7d685c104c72",
   "metadata": {},
   "source": [
    "Disappointing, is the fact that there seems to be no clear trend with areas of higher fountain buffer counts being concentrated above 200 squirrels per buffer. If I were to hazard a guess this is because the locations with lower squirrel populations may happen to be adjacent impermiable areas. These include the likes of lakes or bounding roadways for instance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
